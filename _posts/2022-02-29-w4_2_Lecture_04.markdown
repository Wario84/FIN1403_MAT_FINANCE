---

layout: post
title: "Lecture 4: ..."
author: "Mario H. Gonzalez-Sauri"
date: "2022-02-22"
mermaid: true

---

<!--  FORMAT: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet -->

# Introduction

<!--  https://trends.google.com/trends/explore?date=2012-01-01%202022-02-25&q=%2Fm%2F0jt3_q3,%2Fm%2F01hyh_,Big%20Data  -->


Welcome to the Introduction to Applied Data Science! I'm sure you hace heard the terms, Big Data, Machine Learning and Data Science in the media or perhaps from other peers. It seems that today, that the terms are being used more often. In fact, Data Science and Machine Learning (ML) have reach the Top in the Google index of most frequent terms as fields of study. This pattern has been increasing for the last 10 years, and the trend seem to be upward.

<br> 

  <script type="text/javascript" src="https://ssl.gstatic.com/trends_nrtr/2884_RC01/embed_loader.js"></script>
  <script type="text/javascript">
    trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"/m/0jt3_q3","geo":"","time":"2012-01-01 2022-02-25"},{"keyword":"/m/01hyh_","geo":"","time":"2012-01-01 2022-02-25"},{"keyword":"Big Data","geo":"","time":"2012-01-01 2022-02-25"}],"category":0,"property":""}, {"exploreQuery":"date=2012-01-01%202022-02-25&q=%2Fm%2F0jt3_q3,%2Fm%2F01hyh_,Big%20Data","guestPath":"https://trends.google.com:443/trends/embed/"});
  </script>

<br> 

  Data Science, is an emergent discipline that works in the intersection between statistics, mathematics and computer programming. This new discipline is growing faster due to large amounts of data that we are generating, the maturity of information and communication technologies (ICT) and the high availability of powerful computational systems. Data science differs from statistics, the former justifies the development of algorithms to increase our inferential capabilities. That is, algorithms are needed to estimate certain parameter in a statistical or mathematical model, aiming only to claim a 'causal-effect'. However, the later, justifies the development of algorithms not necessarily for inference but for prediction and complex problem solving [(Efron, 2021)](https://www.cambridge.org/highereducation/books/computer-age-statistical-inference-student-edition/D9E017E05F047BE6A1702D847CB38DB9#overview). For instance, Data Science and Machine Learning (ML) are growing rapidly in intersection with econometrics [(Athey and Imbens, 2019)](https://www.annualreviews.org/doi/abs/10.1146/annurev-economics-080217-053433). Broadly speaking, Data Science and ML uses, statistics, algorithms, powerful numerical methods and computer training to solve problems. An heuristic approach means, that the process of problem solving does not rely only on mathematical induction with a neat [`closed-form'](https://stats.stackexchange.com/questions/70848/what-does-a-closed-form-solution-mean) solution, common in optimization, but also from approximations. This approximations are sometimes better and more cost-efficient given the large amount of data.


But, why all the fuss about around Data Science and ML? and why do we should care? Well, the main reason is because they have great implications for the economy overall and how we function as a society. You have probably heard that algorithms have beated human champtions, in games like [chest](https://www.theguardian.com/sport/2021/feb/12/deep-blue-computer-beats-kasparov-chess-1996) or a more intuitive game Chinese game called [Go](https://deepmind.com/research/case-studies/alphago-the-story-so-far). But far more than that, the field is touching and finding solutions to very relevant problems. Take a look at the following examples:

<br> 

- Environment: A ubiquos concern today is the rapid increase in yearly measures of temperature at a global scale. Indeed, the climate change has impact in our livelihood, economy and overall livelihood. To fight climate change, researchers and policy makers are pooling effort and resourses to find novel ways of addressing this common thread. In this regard the study of [Song and Wang (2018)](https://link-springer-com.mu.idm.oclc.org/article/10.1007/s11356-016-7925-1), uses Big data paired with econometric methods to study how participation in [Global Value Chains](https://iap.unido.org/articles/what-are-global-value-chains-and-why-do-they-matter) affects positively the progress of green technologies. To describe the evolution path and evolution law of regional agricultural sustainable growth, the work of [Song and Wang (2018)](https://link-springer-com.mu.idm.oclc.org/content/pdf/10.1007/s11356-016-7925-1.pdf), combines Big data and ML to develop a framework (model) and give policy recommendations. The seminal paper by [Huntingford and Huntingford, Et Al (2019)](https://iopscience-iop-org.mu.idm.oclc.org/article/10.1088/1748-9326/ab4e55) from Harvard, summarizes well the intersection between Data Science and Climate change. They work show many contributions from ML to the development of Earth System models, Weather forecasting and Climate impacts.


- Cancer:
ML and Data science have prove to be effective method in the field of cancer research. For instance, the study of [Kourou,Exarchos, Et Al. (2015)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000367752600002), applies classification algorithms from ML to detect cancer patients into high or low risk groups. Similarly the work of [Bi, Hosny, Et Al. (2019)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000460475700005), show that ML and AI are effective for the prompt detection of cancer by automating the initial interpretation of radiographic images to asses whether or not to administer an intervention, and subsequent evaluation of patients. A third important example is the research by [Hirasawa, Aoyama, Et Al. (2018)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000435391300008), uses neural networks algorithms to assess thousands of endoscopic images for the evaluation and diagnostics of gastric cancer.

- Transportation: Our cities are growing in density and to ensure economic growth we have to attend the movement of labor within and between cities, regions and counties. In this area, Data Science and ML, are also finding sulution to the complex problem of today's labor mobility. The survey of [Ghofrani, He, Et Al (2018)](https://www-sciencedirect-com.mu.idm.oclc.org/science/article/pii/S0968090X18303395) recollects the mayor contribitions of Data Science on the railway transportation industry from the last 15 years. He shows how Data Science has contributed to solve problems with logistics; such as traffic control systems, train tracking, ticket sales and authomatic fare collection. In Safety, the field has implemented system of incident analysis and rick management. For all these, researcher have applied ML and Data Science for image processing, string and semantic analysis and clustering and classification algorithms amongh many other applications.


- Economics: Data Science, ML and Big data they seem to evolving as new empirical methods in Economics. In particular in the field of econometrics, Data Science contributis with power data manipulation tools, new tools for variable selection and prediction, and more flexible relationships that go beyond the linear model from OLS [(Varian, 2014)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000344365500001). In the same line, [Athey and Imbens (2019)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000483866000026), argue that methods that work in the intersection of ML and econometrics most of the time perform better than the sole use ML for causal inference of average treatment effects and estimation of the counterfactual effects in evidence base policy and consumer choice models. For instance, the survey of [Koum, Xiangrui, Et All. (2019)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000485885700001), revises the developments in the field of financial risk assessment combined with ML, Big Data analysis, network analysis and sentiment analysis. The study of [Gu, Kelly, Et Al. (2020)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000536040400008) compares the use of ML algorithms (trees and neural network) vis vi traditional linear-regresion methods on the problem of asset pricing and show higher performance on the problem of assest pricing. Another interesting example is the research by [Nickerson and Rogers (2014)](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000344365500003), that show the diffusion of Data Science and ML techniques for performing individual level predictions about  supporting candidates and issues or changing their behavior conditional on being targeted with specific campaign interventions. 

# The landscape of Data Science in Economics.

To understand better the scope of Data Science and ML this section presents an overview of the **High Impact Factor** peer-review publications in the field of economics. First, we explore the top 10 publications within the field that have had the highest reach among top-researchers using Data Science. What we can observe is the high overlap, between common empirical disciplines like economics but also a connection to finance, market analysis, and policy making. 

## Top 10 Publications in High Impact Journals.

| Rank | Publication | Journal | Year |
| :---         |     :---:      | :---:|:---|
| 1    | [Big Data: New Tricks for Econometrics](https://www-webofscience-com.mu.idm.oclc.org/wos/woscc/full-record/WOS:000344365500001)             | JOURNAL OF ECONOMIC PERSPECTIVES         |   2014   |
| 2    | [Machine Learning: An Applied Econometric Approach](/wos/woscc/full-record/WOS:000403753100004)             | JOURNAL OF ECONOMIC PERSPECTIVES         | 2017      |
| 3    | [The 'actually existing smart city'](/wos/woscc/full-record/WOS:000351054700003)             | CAMBRIDGE JOURNAL OF REGIONS ECONOMY AND SOCIETY        | 2015      |
| 4    | [The State of Applied Econometrics: Causality and Policy Evaluation](/wos/woscc/full-record/WOS:000403753100001) | JOURNAL OF ECONOMIC PERSPECTIVES    | 2017      |
| 5    | [The forthcoming Artificial Intelligence (AI) revolution: Its impact on society and firms](/wos/woscc/full-record/WOS:000403744000004) | ECONOMETRICS JOURNAL | 2018      |
| 6    | [How effective are neural networks at forecasting and prediction? A review and evaluation](/wos/woscc/full-record/WOS:000077416900010) | JOURNAL OF FORECASTING | 2010  |
| 7    | [How effective are neural networks at forecasting and prediction? A review and evaluation](/wos/woscc/full-record/WOS:000077416900010) | ECONOMETRICS REVIEW | 1998     |
| 8    | [An Empirical Comparison of Machine Learning Models for Time Series Forecasting](wos/woscc/full-record/WOS:000281853600006)| JOUNRAL OF BANKING AND FINANC         | 2010      |
| 9    | [Consumer credit-risk models via >machine-learning algorithms](/wos/woscc/full-record/WOS:000281986300016)| JOUNRAL OF BANKING AND FINANC         | 2010      |
| 10   | [Seeing like a market](/wos/woscc/full-record/WOS:000399771000002)            | SOCIO-ECONOMIC REVIEW         | 2017     |

<!-- https://www.markdownguide.org/extended-syntax/ -->
*Source: Web of Science, 2022*



Indeed, the histogram of publications from the field of Data Science shows that in the last years more economists are adopting these empirical set of skills. What is clear as well, is that the pattern seems to be exponential, with a clear increase in the trend between the years of 2014-2015.



![Web of Science Data Science Publications in Economics](https://github.com/Wario84/idsc_mgs/raw/master/assets/imgs/woc_dataS_ML_rise.png?raw=true)


But what about the sub-disciplines in the field of economics working with Data Science? The top 10 publications show that the field is growing faster in the intersection between empirical economics, finance and policy analysis. The following picture, looks at what are the top sub-disciplines within the field of economics working with data science. What the data is showing is that also fields like management, health economics and operations research are incorporating the use of Data Science for their research.

![Web of Science Data Science Sub-disciplines in Economics](https://github.com/Wario84/idsc_mgs/raw/master/assets/imgs/woc_dataS_ML_disc.png?raw=true)


# The rise of Big Data

In the last decade, we have seem an increase in the number of publications using Data Science in economics. But this is not only given by the growth of the field itself but also due to the large collection of data. In 2014, it was calculated that there are over 2 billion people worldwide are connected to the Internet, 5 billion individuals with mobile phones, but by 2020, it was predicted an increase of 44%, that is 50 billion of devices with access to internet. This example is just from the telecommunication sector, but image how much data we are generating globally in the financial, eCommerce, transportation or health care sectors. 


I is difficult to define what Big Data really is, it seem, somehow to be an "vague-term" that refers to just a large collection of data. Big data, is more a term "used to describe a wide range of concepts: from the technological ability to store, aggregate, and process data, to the cultural shift that is pervasively invading business and society, both drowning in information overload" [@2015DeMauro10.1063/1.4907823]. Their review provides an excellent group of definitions from different authors, that can guide our understanding of what the term encompasses:

## Towards a Definition of Big Data

|    | Definitions of Big Data   |
|:--|:---:|
| 1  | High volume, velocity and variety information assets that demand   cost-effective, innovative forms of information processing for enhanced   insight and decision making.     |
| 2  | The four characteristics defining big data are Volume, Velocity, Variety and Value.         |
| 3  | Complex, unstructured, or large amounts of data.											   |
| 4  | Can be defined using three data characteristics: Cardinality, Continuity and Complexity.	   |
| 5  | Big data is a combination of Volume, Variety, Velocity and Veracity that creates an opportunity for organizations to gain competitive advantage in today’s digitized marketplace. |
| 6  | Extensive datasets, primarily in the characteristics of volume, velocity and/or variety, that require a scalable architecture for efficient storage, manipulation, and analysis. |
| 7  | The storage and analysis of large and or complex data sets using a series of techniques including, but not limited to: NoSQL, MapReduce and machine learning. |
| 8  | The process of applying serious computing power, the latest in machine learning and artificial intelligence, to seriously massive and often highly complex sets of information. |
| 9  | Data that exceeds the processing capacity of conventional database systems.         |
| 10 | Data that cannot be handled and processed in a straightforward manner.              |
| 11 | A dataset that is too big to fit on a screen.									   |
| 12 | Datasets whose size is beyond the ability of typical database software tools to capture, store, manage, and analyze. |
| 13 | The data sets and analytical techniques in applications that are so large and complex that they require advanced and unique data storage, management,   analysis, and visualization technologies. |
| 14 | A cultural, technological, and scholarly phenomenon that rests on the interplay of Technology, Analysis and Mythology. |
| 15 | Phenomenon that brings three key shifts in the way we analyze information that transform how we understand and organize society: 1. More data, 2.   Messier (incomplete) data, 3. Correlation overtakes causality. |
| 16 | Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical   Methods for its transformation into Value. |

**Source: [De Mauro, Andrea and Greco, Et. all, 2015](https://aip-scitation-org.mu.idm.oclc.org/doi/abs/10.1063/1.4907823)**

Indeed, the last definition, 16, provided by De Mauro, Andrea and Greco, Et. all (2015), highlights the "raw-value" of Big Data. Namely, Big Data itself is of no use without a specific set of skills, the skills of *Data Scientist*. Big data, implies large volume of data, but this is not necessary clean and neat as typical [relation database](https://en.wikipedia.org/wiki/Relational_database) used by companies and government. 

## Properties of Big Data

In this section, I try to identify some properties of Big Data, that I have reflected upon my observation and practice.

- *Cost-effective*: In the past, empirical studies use to work with large teams for data collection to implement surveys or deploy field experiments. However, today, Data Scientist with [Web Scrapping](https://www.tandfonline.com/doi/pdf/10.1080/10691898.2020.1787116) skills, can collect with a single computer millions of observations. 

- *Dimension*: A feature of big data is that different from survey data, that only contains a lesser number of variables. Big data typically contain multiple dimensions of variables on the same unit of observation. 

- *Scope*: Another element of Big Data is that not it is able to merge multiple layers of units. That means that Big data is often able to map observations from micro units that is merge with higher order units. You can think of users of cellphones from a district, that are merge with data from their state and even their country. Some authors refers to this as the *granularity* of the data.

- *Unstructured*: However, with the advent of Data Science and ML, more and more scientist are harvesting data available from different databases and websites. This data however, is unstructured and fuzzy, deviates from neat relational databases and requires special Data Science skills to harvest the value out of it. 

- *Magnitude*: Given that Big Data contains occupies a large volume of cyber space, regular computers are not able to process and load the data for analysis. For instance, the work of 
[Qian, (2014)](https://journal.r-project.org/archive/2014-1/RJ-2014-1.pdf#page=57), proposes that a solution to overcome computer overloading is to process the Big Data in a sequential manner. This is an important and somehowe more obvios feature of Big Data, the fact this kind of data is growing exponentially from Gigabyte ($$1024^3$$) to Petabyte ($$1024^7$$), and perhaps shortly to larger files.

# Data Science with **R**

[stackoverflow.co](https://stackoverflow.co/) is a platform of programmers and Data Scientiest that serves 100 million people every month, making it one of the most popular websites in the world. Every year, the platform releases a [survey of the Market of computer languages](https://insights.stackoverflow.com/survey/2021); without exeption, for the last 7 years at least **R** has been listed amont the top-computer used. A great feature of **R** is that everything is free and open-source. That means that you can actually deepen your knowledge by learning how the `functions`, `packages` and `snippets` of code that high-developers perform, are accesible to you. Indeed, In my own practice I have learned a lot digging the `Github` repositories of the packages that I use, even some times I have contributed or builded upon this knowledge.

The community of **R** users is ever evolving, and highly supportive, till the day, [The Comprehensive R Archive Network](https://cran.r-project.org/web/packages/) has around *18990* fully working `packages`. If you are not familiar with what the `packages` are, think of them as *add-ons*. For instance, when you install **R**, it comes with the "so-called", [The R Base Package](https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html). This "out-of-the-shelf" **R** has the main functions that has been used and depured for many year. However, this core package, evolves slowly, and other communities, for instance, "econometricians" and "statistician" develop and publish the state of art methodologies in the form of packages. You can read at the last developments of this powerfull packages and methodologies in the [The R Journal](https://journal.r-project.org/). The versatiligy of R, the collaborative community and state of the art methodologies implemented are powerful reasons to deploy your Data Science skills using **R**. The article of Weston, S. J., & Yee, D. (2017), [Why you should become a useR: A brief introduction to R.](https://www.psychologicalscience.org/observer/why-you-should-become-a-user-a-brief-introduction-to-r), list three main reason why you should learn R.

> Reasons to learn **R**
> > - will always be able to perform the newest statistical analyses as soon as anyone thinks of them;
> > - will fix its bugs quickly and transparently; and
> > - has brought together a community of programming and stats nerds (a.k.a., useRs) that you can turn to for help. 

But if that is not enough encorage to speappen your learning curve and start learning Data Science with **R**, rith away, take a look at the demand for Data Science from the biggest companines in the world.

![Top Companies Working with R](https://1.bp.blogspot.com/-CytixCJHq6w/WGOlcgvciDI/AAAAAAAAFvY/TkNVgC_bA5oEHutbmETCOXnxmzsaOJDzwCLcB/s1600/Companies%2Busing%2BR.png)
**Source: [Deepanshu Bhalla, 2016](https://www.listendata.com/2016/12/companies-using-r.html)**

# The scope of Data Science

<!-- https://www.projectpro.io/data-science-in-python-tutorial/introduction-to-data-science-r -->

## What is a Data Scientist?
It is complicated to integrate all the skills and knowledge that the Data Science field encompasses, symply because the fielt is evolving rapidly. But perhapts, starting out with a simple definition of a Data Scientist, can help us draw some distintions over the work of a programmer and a statistician/econometrician. According to [Hicks and Irizarry, (2018)](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1356747?src=recsys), a Data Scientist has two main features; one, is that they focuss on analysis, but different from a statistician they focuss on how to process and interpret data to answer real-world questions. The second feature, is that have very strong coding skills, they know how to aquire, clean and manipulate data for analysis.


The research question is related to the type of analysis, they jointly determine the strategy of data collection and manipulation. As we will learn in [Lecture 2](), scope of Data science is mostly driven by the use of *inductive* reasoning* to derive conclusions about our data. In plain words, induction means collecting smaller peaces of information to draw a general conclusion(s) about our object of study. Our conclusions are the outcome of the type of analysis that we are conducting. Understanding the type of analysis is paramount to select the right methodoloty. This will be covered in detail in [Lecture 3](). The scope of Data Science is well described from the following diagram.

<!-- Source:https://mermaid.live/edit/#pako:eNp9kt9rwjAQx_-VkL3qoOpTB4PaVl-2sU3ZS-vDLb1qIE1dfghO_N8Xe60TBguFNPf59nL3vZ64aCvkMd8a2O_YOis1Cysp3jxaJ1u9YePxI5uf1sc9srZmiQZ1tNKee2GH0yIDBxsKWf9JyUoengcKpp0uizohS768tLJLTziLiE-IpwpBS70d6ITolOjKtQa2OMApwRnBd3QG4QCqx6irXjejTv4pct4p8iJDK4zcO3kYLiGyKF4NVlL8AcsiBW-vdwoF1mZYs6BmtVQqvqsrccNY8nJh1wovG6NTTjX0RnxI60HJb7jxqldMuzLvgxsBhlEJO3AyJJ8VaWsMqttvF9RHVDyD2EmN7AnB3Di9pHaiIhetbpvg5W_eRdTDUvMRb9A0IKvw55wuuORuhw2WPA6vFdbglSt5qc9B6vcVOMwrGebGY2c8jjh4166OWgxn0mQSwlQaCp5_AAXRyRA -->

<div>
 <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    Scope of Data Science:
    <div class="mermaid">
    graph TD
    A[Question] --> B{Type of Analysis}
    A --> C[Data]
    subgraph ______1;
    C --> D1[Data Aquisition]
    D1 --> D2[Data Cleaning]
    D2 --> D3[Data Storage]
    D3 --> D4[Data Retreaval]
    end
    D4 --> B
    subgraph 2;
    B --> E[Descriptive]
    B --> F[Predictive]
    B --> G[Causal]
    classDef red fill:#fdc
    class AN red
    end    
    subgraph 3; 
    E --> E2[Data Visualization]
    E --> E3[Desc. Statistics]
    E3 --> E4[Correlation]
    E3 --> E5[Meas. Central Tendency]
    F --> F1[Machine Learning]
    G --> G1[Econometrics]
    F1 --> G1
    end

    </div>
</div>

*Source: Based on the Frameworks of [Prakash, Padmapriy, and Kumar (2018)](https://ieeexplore.ieee.org/abstract/document/8473342) and [De Mast, Nuijten
& Kapitan (2021)](https://www.tandfonline.com/doi/full/10.1080/00031305.2021.2023633?casa_token=nozjCh-tQnUAAAAA%3A4Twe3v_PNPvre6MIYw1EgyGZjzp7KoIeh4kTQv2DY_yJTH1aJr9fGhpFL6s0h0_NstgQn5l88eVH-n8)*

<br>

I use the frameworks of [Prakash, Padmapriy, and Kumar (2018)](https://ieeexplore.ieee.org/abstract/document/8473342) and [De Mast, Nuijten
& Kapitan (2021)](https://www.tandfonline.com/doi/full/10.1080/00031305.2021.2023633?casa_token=nozjCh-tQnUAAAAA%3A4Twe3v_PNPvre6MIYw1EgyGZjzp7KoIeh4kTQv2DY_yJTH1aJr9fGhpFL6s0h0_NstgQn5l88eVH-n8) to describe three main processes of Data Science, as described in the previos diagram. 

1. Data: Involves all the process of data management: aquisition, extraction, cleaning, storage and retreaving. In principle, the ideal is that retreaving the data is a requirement for the second step, which is *data analysis*. However, in reality, these two process may end up being non-linear, but, they are an interactive loop. For instance, while conducting analysis, you may want to add a new variable that it was not properly extracted or stored; this happens really often.

2. Type of Analysis: The kind of analysis connects back to the question that we want to address. All Data Science questions *say* something about the relation between a dependent variable $$Y$$ and a set of explanatory variable(s) $$X$$. What we want to say, or in other words, the type of *claim*, determines if the analysis that can be *descriptive*, *predictive* or *causal* (covered in [Lecture 3]()). Descriptive analysis, is characterize by questions such as "What is the current state of affairs?"; "How often, how many, when?" or "What happened?". Predictive analysis, revolves arounnd making forecasts about the outcome variable $$Y$$, with a set of predictors $$X$$-- prediction is the default outcome of ML algorithms. Finally, we have causal analysis uses statistical modelling, econometrics sometimes with mixed with ML, to estimate the effect $$X$$ on the dependent variable $$Y$$. Causal analysis typically is able to responde to "Why $$Y$$ behaves in certan manner?"; the answer is due to the effect of $$X$$. The interest of the last type of analysis is thus quantify or mesaure the effect of $$X$$ on $$Y$$.


3. Methods of Data Sciene: The methods used by Data Science are varios and they are evolving rapidly. 








<br>





3.1 Stereotypical examples
Predicting passenger numbers
A railway company develops an app that helps passengers find empty seats in the train. At the heart
of the application is a correlational predictive model, that predicts the number of passengers in a
train compartment from measured CO2 levels in the air and other ’s. CO2 is a good predictor for the
number of passengers, but it is not a causal influence factor: increasing CO2 levels by artificially
releasing extra CO2 would not increase the number of passengers. The cause-and-effect mechanism
is actually the reverse, with passengers as causal influence and CO2 as the effect.


ne of the challenges in maintenance of systems like trains is timing when a component should be
replaced. If the replacement is scheduled too late, the component breaks down in the field, which
may result in very high costs. Therefore, components are preferably replaced preventively, when this
can be done at a convenient moment and in a convenient place. However, premature replacement
wastes useful lifetime of the component, so the preventive replacement should be scheduled as late
as possible. Predictive maintenance is based on a model that predicts when a breakdown is imminent,
for example, by monitoring a condition such as vibration patterns or the concentration of ferrous
particles in lubrication (Carden and Fanning 2004). In our framework, the solution strategy is driven
by a correlational predictive model , with the probability of a breakdown in the next
epoch, and a predictor such as a vibration characteristic or particles concentration. The
controllable variable is the timing of the replacement. We try to minimize the total expected cost,
which is a weighted average of the cost of a breakdown in the field and the cost of a preventive
replacement (weighed by the probability that a breakdown occurs)


.2 Identifying the ’s
Before a model can be fitted, we first need to identify potential predictors. Often in
machine learning practice, the data for fitting a model are assumed given, and the collection of new
data is deemed impossible or not needed (De Veaux, Hoerl and Snee 2016). Identifying the X’s, then,
boils down to selecting, merging, transforming and rescaling features recorded in the available data
sources into variables to be used in the model-building effort, a task that is described as Feature
engineering in Table 1. Feature engineering is done by human analysts, but the promise of deep-
learning techniques is that stacked neural networks can be trained to recognize structure and features
in raw data automatically, at least within certain domains such as image recognition and natural-
language processing.
In addition to feature engineering, frameworks for data science such as CRISP-DM
(Chapman et al. 2000) recommend to consult domain experts, which is similar to approach number 2
in Table 1 (Experiential knowledge). Domain experts may help identify ’s not yet represented in
available data sources, for which it may yet be possible to obtain data either by initiating new
measurements or by acquiring data sources in which they are represented. Other approaches
suggested in machine learning are techniques such as principal-components analysis, t-SNE and auto
encoders, which facilitate the discovery of candidate ’s by revealing structure in data through
clustering and dimensionality reduction (James, Witten, Hastie and Tibshirani 2013). Such
techniques for unsupervised learning are similar to Exploratory data analysis (approach number 1 in
Table 1).

3.3 Modelling the relationship
Random forests, neural networks, support-vector machines and other models popular in machine
learning do not make a causal claim. The predictions are essentially based on the correlation
(association) structure of and variables: high CO2 levels co-occur with large numbers of
passengers, but they do not cause them, and ferrous particles may predict a breakdown, but they do
not cause it.
This has important ramifications for inference. First, the model should be fitted on a
representative dataset, in which the correlation structure is representative for the correlations in the
target population (the universe of future observations that the model is claimed to predict). This
implies that, in general, experiments are unsuited, since by deliberately setting the levels of the ’s,
they break the correlation structure among the ’s and potentially the . For example, a randomized
controlled experiment studying the effect (sic) of CO2 on the number of passengers would have the
experimenter manipulate CO2 levels according to an experimental design, and then measure the corresponding passenger numbers (note how inappropriate the term effect is in this context of
correlational modelling). By manipulating the CO2 levels, their relationship with passenger numbers
is perturbed, and the algorithm cannot be used to predict passenger numbers in normal situations.
Second, the fitted model can only be trusted within the population from which the dataset was
sampled. The algorithm predicting passengers from CO2 levels does not give reliable predictions for
passenger numbers in different types of train compartments (or for aircraft cabins, boat cabins, etc.).
Instead of causality, machine learning focuses on predictive accuracy and generalizability
(James et al. 2013). The latter refers to the model’s predictive accuracy for a new dataset generated
by the same data generating mechanism. Machine learning uses a variety of metrics for expressing
predictive accuracy, such as the mean-squared error , the coefficient of determination , or the
precision and recall pair. To guard against the detrimental effect overfitting has on generalizability,
machine learning uses the train-test split method and cross validation to regulate a model’s
complexity.












